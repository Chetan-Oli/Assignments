{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46178d5-d60c-4c17-9ede-14dce00fc097",
   "metadata": {},
   "source": [
    "# 21st Feb | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15080d85-8796-498f-b298-71521247f3f4",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ef8bb-ca8b-4a91-8dde-55d9ed6a504e",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of automatically collecting data from websites using software tools or scripts. <br>\n",
    "It involves extracting information from websites by sending requests to web servers and parsing the responses to extract the desired information.\n",
    "\n",
    "Web scraping is used for:\n",
    "\n",
    "1. Business Intelligence: <br>\n",
    "Web scraping is used by businesses to collect and analyze data on competitors, market trends, customer behavior, pricing, and other relevant information. <br>\n",
    "For example: An e-commerce company might use web scraping to track the prices of products on competing websites, analyze customer reviews, and monitor the availability of products.\n",
    "\n",
    "2. Research: <br>\n",
    "Researchers use web scraping to collect data for various academic, scientific, and social research purposes. <br>\n",
    "For example: Social scientists might use web scraping to study online behavior, track sentiment on social media platforms, and monitor news and media sources.\n",
    "\n",
    "3. Content Aggregation: <br>\n",
    "Web scraping is used by content aggregators to collect and organize data from different sources for display on their websites or mobile apps. <br>\n",
    "For example: A news aggregator might use web scraping to collect headlines, summaries, and images from multiple news websites and present them on their own website.\n",
    "\n",
    "Here are three areas where Web Scraping is used to get data:\n",
    "\n",
    "1. E-commerce: <br>\n",
    "Online retailers use web scraping to monitor competitors' prices, inventory levels, and product descriptions. <br>\n",
    "For example: Amazon might use web scraping to collect data on the prices of products on other e-commerce websites to adjust their own pricing strategy.\n",
    "\n",
    "2. Real Estate: <br> \n",
    "Real estate agents and property management companies use web scraping to collect data on properties for sale or rent, including price, location, amenities, and other details. <br>\n",
    "For example: Information of an area can help real estate agents and property management companies make informed decisions about pricing, marketing, and property management.\n",
    "\n",
    "3. Social Media: <br>\n",
    "Social media companies use web scraping to collect data on user behavior, engagement, and sentiment. <br>\n",
    "For example: Twitter might use web scraping to collect data on hashtags, mentions, and user engagement to analyze trends and inform their advertising strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33edcb-8676-4cb7-b902-c4a484857ec9",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ab669-eee9-4683-a967-1406a05df4d6",
   "metadata": {},
   "source": [
    "Answer 2:-\n",
    "\n",
    "Different methods used for Web Scraping are:\n",
    "\n",
    "1. Parsing HTML: <br>\n",
    "This involves analyzing the structure of HTML documents to extract the relevant data. <br>\n",
    "This can be done using tools like BeautifulSoup, which provide a simple interface for parsing HTML.\n",
    "\n",
    "For Example: Extracting the titles of articles from a news website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0d77c9-375a-41ab-974b-8294c186cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.example.com/news'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "articles = soup.find_all('h2', {'class': 'article-title'})\n",
    "\n",
    "for article in articles:\n",
    "    print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1109e-03c4-48a9-ad81-f71b00671662",
   "metadata": {},
   "source": [
    "2. Using APIs: <br>\n",
    "Many websites provide APIs (Application Programming Interfaces) that allow developers to access their data programmatically. <br>\n",
    "APIs provide a structured and standardized way of accessing data, making it easier to extract the desired information. <br>\n",
    "For example: If you want to extract weather data from a weather website, you can use their API to access the data and extract the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f52c67d-9211-4d47-b5f5-74045a82e5e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m movie \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSearch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(movie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m], movie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Search'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.omdbapi.com/'\n",
    "params = {\n",
    "    'apikey': 'your_api_key',\n",
    "    's': 'star wars',\n",
    "    'type': 'movie'\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "for movie in data['Search']:\n",
    "    print(movie['Title'], movie['Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff87510-7365-4dd9-a934-96357cbf84b7",
   "metadata": {},
   "source": [
    "3. Automated web browsing: <br>\n",
    "This method involves using tools such as Selenium and Puppeteer to automate web browsing and extract data from websites. <br>\n",
    "Automated web browsing can be useful for scraping data from websites that require user authentication or have complex user interfaces. <br>\n",
    "For example: If you want to extract data from a website that requires user authentication, you can use Selenium to automate the login process and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62aefee2-f3b8-4352-97cf-bd7849cd8ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /opt/conda/lib/python3.10/site-packages (from selenium) (1.26.13)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.10/site-packages (from selenium) (2022.12.7)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.9.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cda4c8e-d01e-4ac7-9c6d-df58299caf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_879/606707481.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/path/to/chromedriver')\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service /home/jovyan/.cache/selenium/chromedriver/linux64/112.0.5615.49/chromedriver unexpectedly exited. Status code was: 127\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.example.com/product/1234/reviews\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/path/to/chromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      7\u001b[0m reviews \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements_by_class_name(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview-text\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:84\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     81\u001b[0m     service \u001b[38;5;241m=\u001b[39m Service(executable_path, port, service_args, service_log_path)\n\u001b[1;32m     82\u001b[0m service\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder\u001b[38;5;241m.\u001b[39mget_path(service, options)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesired_capabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py:101\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    105\u001b[0m         command_executor\u001b[38;5;241m=\u001b[39mChromiumRemoteConnection(\n\u001b[1;32m    106\u001b[0m             remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    113\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/selenium/webdriver/common/service.py:100\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_process_still_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connectable():\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/selenium/webdriver/common/service.py:113\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unexpectedly exited. Status code was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /home/jovyan/.cache/selenium/chromedriver/linux64/112.0.5615.49/chromedriver unexpectedly exited. Status code was: 127\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://www.example.com/product/1234/reviews'\n",
    "driver = webdriver.Chrome('/path/to/chromedriver')\n",
    "driver.get(url)\n",
    "\n",
    "reviews = driver.find_elements_by_class_name('review-text')\n",
    "\n",
    "for review in reviews:\n",
    "    print(review.text)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197bc73-5a19-4fc2-9eed-c1f118387407",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f912639-800f-4efc-b6ce-b364069acfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae5c35a-30d1-4311-881e-ccc8b0ff4f09",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972c6c9-3f74-42e7-ba79-e7b6d61c0cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4bd329f-bc54-4c56-a1a2-f3efbbd99245",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde57a6-0d0c-4a70-a445-b0fcba6535c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
